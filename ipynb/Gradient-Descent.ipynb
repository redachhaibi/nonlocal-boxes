{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import non_local_boxes\n",
    "\n",
    "# Sugar coating for reloading\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Maximization under constraints\n",
    "\n",
    "As we maximize under constraints, we need the orthogonal projection onto the unit hypercube $[0,1]^{32}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_wiring_basic(W):  # W is a 32xn tensor\n",
    "    W = torch.maximum(W, torch.zeros_like(W))  # it outputs the element-wise maximum\n",
    "    W = torch.minimum(W, torch.ones_like(W))   # similarly for minimum\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_wiring(W):  # W is a 32xn tensor\n",
    "    W = torch.maximum(W, torch.zeros_like(W))  # it outputs the element-wise maximum\n",
    "    W = torch.minimum(W, torch.ones_like(W))   # similarly for minimum\n",
    "\n",
    "    # Then we verify the 4 equalities of (5)\n",
    "    for alpha in range(non_local_boxes.evaluate.nb_columns):\n",
    "        #1\n",
    "        if abs(W[0, alpha] - W[1, alpha]) <= abs(W[8, alpha] - W[9, alpha]):\n",
    "            W[0, alpha] = (W[0, alpha]+W[1, alpha])/2\n",
    "            W[1, alpha] = W[0, alpha]\n",
    "        else:\n",
    "            W[8, alpha] = (W[8, alpha]+W[9, alpha])/2\n",
    "            W[9, alpha] = W[8, alpha]\n",
    "            \n",
    "        #2\n",
    "        if abs(W[2, alpha] - W[3, alpha]) <= abs(W[10, alpha] - W[11, alpha]):\n",
    "            W[2, alpha] = (W[2, alpha]+W[3, alpha])/2\n",
    "            W[3, alpha] = W[2, alpha]\n",
    "        else:\n",
    "            W[10, alpha] = (W[10, alpha]+W[11, alpha])/2\n",
    "            W[11, alpha] = W[10, alpha]\n",
    "            \n",
    "        #3\n",
    "        if abs(W[4, alpha] - W[5, alpha]) <= abs(W[12, alpha] - W[13, alpha]):\n",
    "            W[4, alpha] = (W[4, alpha]+W[5, alpha])/2\n",
    "            W[5, alpha] = W[4, alpha]\n",
    "        else:\n",
    "            W[12, alpha] = (W[12, alpha]+W[13, alpha])/2\n",
    "            W[13, alpha] = W[12, alpha]\n",
    "                \n",
    "        #4\n",
    "        if abs(W[6, alpha] - W[7, alpha]) <= abs(W[14, alpha] - W[15, alpha]):\n",
    "            W[6, alpha] = (W[6, alpha]+W[7, alpha])/2\n",
    "            W[7, alpha] = W[6, alpha]\n",
    "        else:\n",
    "            W[14, alpha] = (W[14, alpha]+W[15, alpha])/2\n",
    "            W[15, alpha] = W[14, alpha]\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = non_local_boxes.utils.random_wiring(3).detach()\n",
    "torch.abs(W[0,:]-W[1,:]) <= torch.ones(3)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = non_local_boxes.utils.W_BS09(non_local_boxes.evaluate.nb_columns).detach()\n",
    "P = non_local_boxes.utils.SR\n",
    "Q = non_local_boxes.utils.PR\n",
    "float(non_local_boxes.evaluate.phi_flat(W, P, Q)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(starting_W, P, Q, learning_rate = 2, nb_steps = 20):\n",
    "    external_grad = torch.ones(non_local_boxes.evaluate.nb_columns)\n",
    "    W = starting_W\n",
    "    for i in range(nb_steps):\n",
    "        non_local_boxes.evaluate.phi_flat(W, P, Q).backward(gradient=external_grad)\n",
    "        W = projected_wiring(W + learning_rate*W.grad).detach()  # create a brand new tensor, forgeting the previous gradient\n",
    "        W.requires_grad=True\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "starting_W = non_local_boxes.utils.random_wiring(non_local_boxes.evaluate.nb_columns)\n",
    "P = non_local_boxes.utils.SR\n",
    "Q = non_local_boxes.utils.PR\n",
    "learning_rate = 2\n",
    "nb_steps = 20\n",
    "\n",
    "\n",
    "# Gradient Descent\n",
    "print(\"(Iterating...)\")\n",
    "tic = time.time()\n",
    "gradient_descent(starting_W, P, Q, learning_rate, nb_steps)\n",
    "toc = time.time()\n",
    "\n",
    "\n",
    "# Result\n",
    "list = non_local_boxes.evaluate.phi_flat(W, P, Q).detach().numpy()\n",
    "print(\"\")\n",
    "print(\"Number of tested wirings: \", len(list))\n",
    "print(\"--> Min:    \", min(list))\n",
    "print(\"--> Av.:    \", sum(list)/len(list))\n",
    "index, value = max(enumerate(list), key=lambda x: x[1])\n",
    "best_wiring = W[:,index].detach().numpy()\n",
    "print(\"--> Max:    \", value)\n",
    "print(\"Duration: \", (toc-tic)*1e0, \"s\")\n",
    "print(\"\")\n",
    "print(\"-----\")\n",
    "print(\"The best wiring is:\\n\", best_wiring)\n",
    "print(\"-----\")\n",
    "non_local_boxes.utils.wiring_to_functions(best_wiring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.24272715, 0.12160056, 0.9498826 , 0.02322189, 0.39412087,\n",
    "       0.39412087, 0.83709   , 0.52627677, 0.        , 0.        ,\n",
    "       1.        , 1.        , 0.        , 0.        , 1.        ,\n",
    "       1.        , 1.        , 0.        , 1.        , 0.        ,\n",
    "       1.        , 0.        , 1.        , 0.        , 1.        ,\n",
    "       0.        , 1.        , 0.        , 1.        , 0.        ,\n",
    "       1.        , 0.        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.t(torch.tensor([[0., 0., 1. , 0., 0.,\n",
    "       0., 1.   , 0.5 , 0.        , 0.        ,\n",
    "       1.        , 1.        , 0.        , 0.        , 1.        ,\n",
    "       1.        , 1.        , 0.        , 1.        , 0.        ,\n",
    "       1.        , 0.        , 1.        , 0.        , 1.        ,\n",
    "       0.        , 1.        , 0.        , 1.        , 0.        ,\n",
    "       1.        , 0.        ]]))\n",
    "\n",
    "P = non_local_boxes.utils.SR\n",
    "Q = non_local_boxes.utils.PR\n",
    "float(non_local_boxes.evaluate.phi_flat(W, P, Q)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_boxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc316f9929218361e7d645a202e8c2b79a1175dfee9b41dd9aa3806e9995da6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
