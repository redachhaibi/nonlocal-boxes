{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import non_local_boxes\n",
    "import numpy as np\n",
    "from IPython.display import clear_output   # in order to clear the print output\n",
    "import time\n",
    "\n",
    "# Sugar coating for reloading\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('svg')   # in ordert to have unblurred pictures\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = torch.zeros(32, 32)\n",
    "for i in range(32):\n",
    "    M1[i,i]=1\n",
    "M1[0,0]=0.5\n",
    "M1[0,1]=0.5\n",
    "M1[1,0]=0.5\n",
    "M1[1,1]=0.5\n",
    "\n",
    "M2 = torch.zeros(32, 32)\n",
    "for i in range(32):\n",
    "    M2[i,i]=1\n",
    "M2[8,8]=0.5\n",
    "M2[8,9]=0.5\n",
    "M2[9,8]=0.5\n",
    "M2[9,9]=0.5\n",
    "\n",
    "M3 = torch.zeros(32, 32)\n",
    "for i in range(32):\n",
    "    M3[i,i]=1\n",
    "M3[2,2]=0.5\n",
    "M3[2,3]=0.5\n",
    "M3[3,2]=0.5\n",
    "M3[3,3]=0.5\n",
    "\n",
    "M4 = torch.zeros(32, 32)\n",
    "for i in range(32):\n",
    "    M4[i,i]=1\n",
    "M4[10,10]=0.5\n",
    "M4[10,11]=0.5\n",
    "M4[11,10]=0.5\n",
    "M4[11,11]=0.5\n",
    "\n",
    "M5 = torch.zeros(32, 32)\n",
    "for i in range(32):\n",
    "    M5[i,i]=1\n",
    "M5[4,4]=0.5\n",
    "M5[4,5]=0.5\n",
    "M5[5,4]=0.5\n",
    "M5[5,5]=0.5\n",
    "\n",
    "M6 = torch.zeros(32, 32)\n",
    "for i in range(32):\n",
    "    M6[i,i]=1\n",
    "M6[12,12]=0.5\n",
    "M6[12,13]=0.5\n",
    "M6[13,12]=0.5\n",
    "M6[13,13]=0.5\n",
    "\n",
    "M7 = torch.zeros(32, 32)\n",
    "for i in range(32):\n",
    "    M7[i,i]=1\n",
    "M7[6,6]=0.5\n",
    "M7[6,7]=0.5\n",
    "M7[7,6]=0.5\n",
    "M7[7,7]=0.5\n",
    "\n",
    "M8 = torch.zeros(32, 32)\n",
    "for i in range(32):\n",
    "    M8[i,i]=1\n",
    "M8[14,14]=0.5\n",
    "M8[14,15]=0.5\n",
    "M8[15,14]=0.5\n",
    "M8[15,15]=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_wiring(W):  # W is a 32xn tensor\n",
    "    W = torch.maximum(W, torch.zeros_like(W))  # it outputs the element-wise maximum\n",
    "    W = torch.minimum(W, torch.ones_like(W))   # similarly for minimum\n",
    "\n",
    "    T1 = (torch.abs(W[0,:]-W[1,:]) <= torch.abs(W[8, :] - W[9, :]))\n",
    "    W = T1*torch.tensordot(M1, W, dims=1) + torch.logical_not(T1)*torch.tensordot(M2, W, dims=1)\n",
    "    \n",
    "    T2 = (torch.abs(W[2,:]-W[3,:]) <= torch.abs(W[10, :] - W[11, :]))\n",
    "    W = T2*torch.tensordot(M3, W, dims=1) + torch.logical_not(T2)*torch.tensordot(M4, W, dims=1)\n",
    "\n",
    "    T3 = (torch.abs(W[4,:]-W[5,:]) <= torch.abs(W[12, :] - W[13, :]))\n",
    "    W = T3*torch.tensordot(M5, W, dims=1) + torch.logical_not(T3)*torch.tensordot(M6, W, dims=1)\n",
    "\n",
    "    T4 = (torch.abs(W[6,:]-W[7,:]) <= torch.abs(W[14, :] - W[15, :]))\n",
    "    W = T4*torch.tensordot(M7, W, dims=1) + torch.logical_not(T4)*torch.tensordot(M8, W, dims=1)\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(starting_W, P, Q, learning_rate, nb_iterations = 400, tolerance=1e-6):\n",
    "    m = non_local_boxes.evaluate.nb_columns\n",
    "    external_grad = torch.ones(m)\n",
    "    W = starting_W\n",
    "    for _ in range(nb_iterations):\n",
    "        Wold = W\n",
    "        non_local_boxes.evaluate.phi_flat(W, P, Q).backward(gradient=external_grad)\n",
    "        W = projected_wiring(W + learning_rate*W.grad).detach() \n",
    "        if (torch.max(torch.abs(W-Wold)) < tolerance):   return W\n",
    "        W.requires_grad=True\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR = non_local_boxes.utils.PR\n",
    "SR = non_local_boxes.utils.SR\n",
    "I = non_local_boxes.utils.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.39\n",
    "q=0.6\n",
    "P = p*PR +q*SR + (1-p-q)*I\n",
    "BoxProduct = non_local_boxes.evaluate.phi_flat\n",
    "\n",
    "m = non_local_boxes.evaluate.nb_columns\n",
    "alpha = 0.01\n",
    "K=int(1e4)\n",
    "epsilon=1e-6\n",
    "\n",
    "W = gradient_descent(\n",
    "    starting_W=non_local_boxes.utils.random_wiring(m),\n",
    "    P=P,\n",
    "    Q=P,\n",
    "    learning_rate=alpha,\n",
    "    nb_iterations=K,\n",
    "    tolerance=epsilon\n",
    ")\n",
    "histogramGD = BoxProduct(W, P, P).tolist()\n",
    "\n",
    "#plt.hist(histogramGD, bins=50, label=\"Gradient Descent (p=\"+str(p)+\", q=\"+str(q)+\", α=\"+str(alpha)+\", K=\"+str(K)+\", m=10^\"+str(int(np.log10(m)))+\", ε=10^\"+str(int(np.log10(epsilon)))+\")\")\n",
    "plt.hist(histogramGD, bins=50, label=\"Gradient Descent (α=\"+str(alpha)+\", K=10^\"+str(int(np.log10(K)))+\", ε=10^\"+str(int(np.log10(epsilon)))+\", m=10^\"+str(int(np.log10(m)))+\")\")\n",
    "#plt.xlabel(\"CHSH-value\")\n",
    "plt.xlabel(\"$\\Phi(\\mathsf{W}_{{out}})$\")\n",
    "plt.ylabel(\"Number of reruns\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "#plt.title(\"Histogram of the different results with a random initialization (with $\\mathbf{P}=(\\mathbf{PR}+\\mathbf{SR})/2$, total: \"+str(N)+\" occurences)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(histogramGD)):\n",
    "    print(str(histogramGD[j])+\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "    \\alpha^*_k = \\argmax_\\alpha \\phi(x_k + \\alpha \\nabla \\phi(x_k))\\\\\n",
    "    x_{k+1}= \\texttt{proj}(x_k + \\alpha^*_k \\nabla \\phi(x_k))\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_list(L, phi):\n",
    "    j=0\n",
    "    while j<len(L):\n",
    "        if j!=0 and phi[L[j-1]]<phi[L[j]]:\n",
    "            L[j-1],L[j]=L[j],L[j-1]\n",
    "            j-=2\n",
    "        j+=1\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi=[0.1, 0.3, 0, 10, 9, 0.5]\n",
    "L = [*range(len(phi))]\n",
    "L = reorder_list(L, phi)\n",
    "print([phi[k] for k in L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_columns(W, P, Q, integer):\n",
    "    if integer==0: return non_local_boxes.utils.random_wiring(m).detach()\n",
    "    # L is the list of the \"best\" indexes of the columns of W\n",
    "    # At the begining, we take the first indexes of W\n",
    "    # We will change the list L by comparing the value at the other indexes\n",
    "    # When we add a term to L, we also remove the \"worst\" one, and we re-order the list L\n",
    "    L = [*range(integer)]\n",
    "    # phi is the list of values:\n",
    "    phi= non_local_boxes.evaluate.phi_flat(W,P,Q).tolist()\n",
    "    # we re-order the list L:\n",
    "    L = reorder_list(L, phi)\n",
    "    for i in range(integer,non_local_boxes.evaluate.nb_columns):\n",
    "        if phi[i]>phi[L[-1]]:\n",
    "            L[-1]=i # we remove and replace the worst index\n",
    "            L = reorder_list(L, phi)\n",
    "\n",
    "    W_new = non_local_boxes.utils.random_wiring(m).detach()\n",
    "    for k in range(integer):\n",
    "        W_new[:,L[k]] = W[:,L[k]] # we keep only the best ones\n",
    "\n",
    "    return W_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_with_resets(P, Q, LS_iterations, K_reset, chi):\n",
    "    m = non_local_boxes.evaluate.nb_columns\n",
    "    phi_flat = non_local_boxes.evaluate.phi_flat\n",
    "    #W = non_local_boxes.utils.random_wiring(m)\n",
    "    W = torch.zeros(32,m)\n",
    "    external_grad = torch.ones(m)\n",
    "    Krange=range(K_reset)\n",
    "    LSrange=range(LS_iterations)\n",
    "    \n",
    "    for j in range(0,int(1/chi)):\n",
    "        # Reset:\n",
    "        W = select_best_columns(W, P, Q, min(m, int(j*m*chi))).detach()\n",
    "        W.requires_grad=True\n",
    "\n",
    "        # At the end, we do a lot of steps:\n",
    "        if j==int(1/chi)-1: Krange=range(10*K_reset)\n",
    "\n",
    "        # Line search:\n",
    "        for _ in Krange:\n",
    "            phi_flat(W, P, Q).backward(gradient=external_grad)\n",
    "            gradient=W.grad\n",
    "            alpha = torch.ones(m)*0.01\n",
    "            for _ in LSrange:\n",
    "                Gains = phi_flat(W, P, Q)\n",
    "                Gains_new = phi_flat(W + alpha*gradient, P, Q)\n",
    "                mask = 0.0 + (Gains>Gains_new)\n",
    "                alpha = 0.5*mask*alpha + 1.7*(1-mask)*alpha\n",
    "            W = projected_wiring(W + alpha*gradient).detach()\n",
    "            W.requires_grad=True\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.39\n",
    "q=0.6\n",
    "P = p*PR +q*SR + (1-p-q)*I\n",
    "BoxProduct = non_local_boxes.evaluate.phi_flat\n",
    "\n",
    "m = non_local_boxes.evaluate.nb_columns\n",
    "K_reset=100\n",
    "chi = 0.0003\n",
    "LS_iterations = 7\n",
    "\n",
    "W=line_search_with_resets(\n",
    "    P, \n",
    "    P, \n",
    "    LS_iterations=LS_iterations, \n",
    "    K_reset=K_reset, \n",
    "    chi=chi\n",
    "    )\n",
    "histogramLS = BoxProduct(W, P, P).tolist()\n",
    "\n",
    "#plt.hist(histogramGD, bins=50, label=\"Gradient Descent (p=\"+str(p)+\", q=\"+str(q)+\", α=\"+str(alpha)+\", K=\"+str(K)+\", m=10^\"+str(int(np.log10(m)))+\", ε=10^\"+str(int(np.log10(epsilon)))+\")\")\n",
    "plt.hist(histogramLS, bins=50, color='purple', label=\"Line Search ($K_{reset}$=\"+str(K_reset)+\", χ=\"+str(chi)+\", m=10^\"+str(int(np.log10(m)))+\", M=\"+str(LS_iterations)+\")\")\n",
    "#plt.xlabel(\"CHSH-value\")\n",
    "plt.xlabel(\"$\\Phi(\\mathsf{W}_{{out}})$\")\n",
    "plt.ylabel(\"Number of reruns\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "#plt.title(\"Histogram of the different results with a random initialization (with $\\mathbf{P}=(\\mathbf{PR}+\\mathbf{SR})/2$, total: \"+str(N)+\" occurences)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(histogramLS)):\n",
    "    print(str(histogramLS[j])+\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_with_resets(P, Q, LS_iterations, K_reset, chi, K, epsilon=1e-6):\n",
    "    m = non_local_boxes.evaluate.nb_columns\n",
    "    phi_flat = non_local_boxes.evaluate.phi_flat\n",
    "    W = non_local_boxes.utils.random_wiring(m)\n",
    "    external_grad = torch.ones(m)\n",
    "    \n",
    "    for j in range(1,int(1/chi)+1):\n",
    "        # reset\n",
    "        for _ in range(K_reset):\n",
    "            # line search \n",
    "            phi_flat(W, P, Q).backward(gradient=external_grad)\n",
    "            Wgrad = W.grad\n",
    "            alpha = torch.ones(m)*0.01\n",
    "            for _ in range(LS_iterations):\n",
    "                Gains = phi_flat(W, P, Q)\n",
    "                Gains_new = phi_flat(W + alpha*Wgrad, P, Q)\n",
    "                mask = 0.0 + (Gains>Gains_new)\n",
    "                alpha = 0.5*mask*alpha + 1.7*(1-mask)*alpha\n",
    "            W = projected_wiring(W + alpha*Wgrad).detach()\n",
    "            W.requires_grad=True\n",
    "        W = select_best_columns(W, P, Q, min(m, int(j*m*chi))).detach()\n",
    "        W.requires_grad=True\n",
    "    \n",
    "    #W = gradient_descent(W, P, Q, alpha*0.1, nb_iterations = K, tolerance=epsilon)\n",
    "\n",
    "    return W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_boxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
