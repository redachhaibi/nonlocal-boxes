{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5V9MDDvf4d8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZpskfIl4PHX"
   },
   "source": [
    "# I. Basic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pM2SKw57isus",
    "outputId": "1cb6c881-8e1a-429b-fd16-1504bda0d437"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha=0.5      # For Lp balls\n",
    "\n",
    "def Loss(q):\n",
    "  alpha_norms = torch.sum( torch.abs(q)**alpha, dim=1 )**(1/alpha)\n",
    "  q_projected = q/alpha_norms[:,None]\n",
    "  return torch.norm( q-q_projected )**2\n",
    "\n",
    "N = 200\n",
    "T = 50\n",
    "dt = 0.05\n",
    "plot_flag = False\n",
    "\n",
    "p = torch.zeros( [N, 2] )\n",
    "theta = 2*torch.pi*torch.linspace(0,1, N)\n",
    "q = torch.vstack( [torch.cos(theta), torch.sin(theta)] ).T\n",
    "q = q+0.1*torch.randn( [N,2] )\n",
    "#\n",
    "print(\"Shapes of positions and momenta vectors:\")\n",
    "print( q.shape )\n",
    "print( p.shape )\n",
    "\n",
    "print(\"Iterating...\")\n",
    "tic = time.time()\n",
    "for t in range(T):\n",
    "  np_q = q.clone().detach().numpy()\n",
    "  np_p = p.clone().detach().numpy()\n",
    "  if plot_flag:\n",
    "    plt.scatter( np_q[:,0], np_q[:,1] )\n",
    "    plt.plot( np_q[:,0], np_q[:,1], 'r' )\n",
    "    for i in range(N):\n",
    "      plt.arrow(x=np_q[i,0], y=np_q[i,1], dx=np_p[i,0]*0.2, dy=np_p[i,1]*0.2, width=.01) \n",
    "    plt.show()\n",
    "  # Autograd for Loss\n",
    "  q.requires_grad = True\n",
    "  L = Loss(q)\n",
    "  L.backward()\n",
    "  dL = q.grad\n",
    "  # Making step\n",
    "  q = torch.tensor( q - dt*dL, requires_grad=True)\n",
    "  p = torch.tensor( -dL, requires_grad=True)\n",
    "# end for\n",
    "toc = time.time()\n",
    "print( \"Total time: \", toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NG8TMGpf6kbu"
   },
   "source": [
    "# II. Gradient descent with line search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RDpSdmYJ6rP6",
    "outputId": "741e64e2-6308-4556-d713-6ba6b9646c0a"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha=0.5      # For Lp balls\n",
    "\n",
    "def Loss_vectorized(q):\n",
    "  alpha_norms = torch.sum( torch.abs(q)**alpha, dim=1 )**(1/alpha)\n",
    "  q_projected = q/alpha_norms[:,None]\n",
    "  return torch.norm( q-q_projected, dim=1 )**2\n",
    "\n",
    "N = 200\n",
    "T = 50\n",
    "dt = 0.05\n",
    "plot_flag = False\n",
    "\n",
    "p = torch.zeros( [N, 2] )\n",
    "theta = 2*torch.pi*torch.linspace(0,1, N)\n",
    "q = torch.vstack( [torch.cos(theta), torch.sin(theta)] ).T\n",
    "q = q+0.1*torch.randn( [N,2] )\n",
    "#\n",
    "print(\"Shapes of positions and momenta vectors:\")\n",
    "print( q.shape )\n",
    "print( p.shape )\n",
    "\n",
    "print(\"Iterating...\")\n",
    "tic = time.time()\n",
    "Losses = None\n",
    "step_damping = torch.ones( [N] )\n",
    "for t in range(T):\n",
    "  np_q = q.clone().detach().numpy()\n",
    "  np_p = p.clone().detach().numpy()\n",
    "  if plot_flag:\n",
    "    plt.scatter( np_q[:,0], np_q[:,1] )\n",
    "    plt.plot( np_q[:,0], np_q[:,1], 'r' )\n",
    "    for i in range(N):\n",
    "      plt.arrow(x=np_q[i,0], y=np_q[i,1], dx=np_p[i,0]*0.2, dy=np_p[i,1]*0.2, width=.01) \n",
    "    plt.xlim( (-1.5, 1.5) )\n",
    "    plt.ylim( (-1.5, 1.5) )\n",
    "    plt.show()\n",
    "  # Autograd for Loss\n",
    "  q.requires_grad = True\n",
    "  Losses_old = Losses\n",
    "  Losses = Loss_vectorized(q)\n",
    "  if Losses_old is not None:\n",
    "    mask = 0.0+( Losses > Losses_old )\n",
    "    step_damping  -= 0.5*step_damping*mask\n",
    "    q = torch.tensor( mask[:, None]*q_old + (1-mask[:,None])*q, requires_grad=True)\n",
    "    Losses = Loss_vectorized(q)\n",
    "  L = torch.sum( Losses )\n",
    "  L.backward()\n",
    "  dL = q.grad\n",
    "  # Making step with line search\n",
    "  q_old = q\n",
    "  q = torch.tensor( q - dt*step_damping[:,None]*dL, requires_grad=True)\n",
    "  p = torch.tensor( -dL, requires_grad=True)\n",
    "# end for\n",
    "toc = time.time()\n",
    "print( \"Total time: \", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "HDuczuwxjBqd",
    "outputId": "a6d1f2be-1d0b-45da-d800-c2e4138fcff7"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha=0.5      # For Lp balls\n",
    "k_spring = 1.0 # Force constant in Hooke's law\n",
    "tresh    = 2.0 # Treshold for gradient\n",
    "\n",
    "def Hamiltonian(p,q):\n",
    "  kinetic_energy = torch.norm(p)**2\n",
    "  potential      = torch.norm( q-torch.roll(q, 1, 0) )**2\n",
    "  return 0.5*kinetic_energy + 0.5*k_spring*potential\n",
    "\n",
    "def Loss(q):\n",
    "  alpha_norms = torch.sum( torch.abs(q)**alpha, dim=1 )**(1/alpha)\n",
    "  #print( alpha_norms.shape )\n",
    "  q_projected = q/alpha_norms[:,None]\n",
    "  #print( q_projected.shape )\n",
    "  return torch.norm( q-q_projected )**2\n",
    "  \n",
    "N = 200\n",
    "T = 50\n",
    "dt = 0.05\n",
    "plot_flag = True\n",
    "\n",
    "p = torch.zeros( [N, 2] )\n",
    "theta = 2*torch.pi*torch.linspace(0,1, N)\n",
    "q = torch.vstack( [torch.cos(theta), torch.sin(theta)] ).T\n",
    "q = q+0.1*torch.randn( [N,2] )\n",
    "#\n",
    "print(\"Shapes of positions and momenta vectors:\")\n",
    "print( q.shape )\n",
    "print( p.shape )\n",
    "\n",
    "print(\"Iterating...\")\n",
    "tic = time.time()\n",
    "for t in range(T):\n",
    "  np_q = q.clone().detach().numpy()\n",
    "  np_p = p.clone().detach().numpy()\n",
    "  if plot_flag:\n",
    "    plt.scatter( np_q[:,0], np_q[:,1] )\n",
    "    plt.plot( np_q[:,0], np_q[:,1], 'r' )\n",
    "    for i in range(N):\n",
    "      plt.arrow(x=np_q[i,0], y=np_q[i,1], dx=np_p[i,0]*0.2, dy=np_p[i,1]*0.2, width=.01) \n",
    "    plt.show()\n",
    "  # Autograd for Hamiltonian\n",
    "  p.requires_grad = True\n",
    "  q.requires_grad = True\n",
    "  H = Hamiltonian(p,q)\n",
    "  H.backward()\n",
    "  dH_over_dp = p.grad\n",
    "  dH_over_dq = q.grad\n",
    "  # Autograd for Loss\n",
    "  q.requires_grad = True\n",
    "  L = Loss(q)\n",
    "  L.backward()\n",
    "  dL = q.grad\n",
    "  # Hamiltonian step\n",
    "  dq_Hamiltonian = p - dL # dH_over_dp\n",
    "  dp_Hamiltonian = -dH_over_dq\n",
    "  # Hybrid step\n",
    "  scalar_products = torch.sum( dL * dq_Hamiltonian, dim=1)\n",
    "  dL_perp  = torch.vstack( [-dL[:,1], dL[:,0]] ).T\n",
    "  #print( dL_perp.shape )\n",
    "  #print( scalar_products.shape )\n",
    "  dq_total = -dL + 0.01*dL_perp*scalar_products[:,None]\n",
    "  dq_total = -dL\n",
    "  #indices = torch.norm(dL, dim=1) < tresh\n",
    "  #print( indices.shape )\n",
    "  #dq_total = -dL*indices[:,None]\n",
    "  # Making step\n",
    "  q = torch.tensor( q + dt*dq_total, requires_grad=True)\n",
    "  p = torch.tensor( p + dt*dp_Hamiltonian, requires_grad=True)\n",
    "# end for\n",
    "toc = time.time()\n",
    "print( \"Total time: \", toc-tic)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv_virgile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "20ffe168062e69c3bc628ac75a1b32b64555bb69f5991e6f3ffca8e6af4b667f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
